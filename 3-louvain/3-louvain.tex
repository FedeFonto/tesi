
\section{Louvain Algorithm}
The Louvain algorithm is a greedy modularity optimization techniques created from a team of researcher from Vincent D. Blondel, Jean-Loup Guillaume, Renaud Lambiotte and Etienne Lefebvre in the 2008 \cite{Blondel_2008}. The algorithm bears the name of the university to which they belong to, i.e. \textit{UniversitÃ© Catholique de Louvain}.
In 2008, the fastest algorithm presented in the literature was the one proposed by Clauset et al. \cite{Clauset_2004}, but the biggest graph at the time that was analysed has 5.5 million users. This was a not so big graph even at the time. For example, Facebook in 2008 has 64 million active users, more than ten times the size of the biggest analyzed graph. This algorithm was proposed to resolve this scaling problem:  indeed the first version of this algorithm identifying communities in a 118 million nodes network in 152 minutes \cite{Blondel_2008}. From that year, many improvement was made and some parallel versions were proposed. 
This algorithm and its parallel version is the main topic of this thesis. The algorithm is very popular due to his simplicity, efficiency and overall precision.
In this chapter, we present the sequential algorithm in details and some optimization technique presented in the literature.
Then we present the parallel version of the algorithm, focusing on the implementations dedicated to the GPU.
\subsection{Description}
\begin{figure}
	\centering
	\hspace*{-1em}
	\includegraphics[width=1\linewidth]{0-resources/blondel_scheme}
	\caption{Scheme of the Louvain algorithm. This image is reprinted from \cite{Blondel_2008}.}
	\label{fig:blondelscheme}
\end{figure}
This greedy algorithm its quite simple. There are two phases that are repeated iteratively: the optimization phase and the aggregation phase. At the start of the optimization, each nodes is assigned to its self-community, i.e. each node belongs to a community composed by only itself. 
In the first phase, for each node $i$, we evaluate for each community $j$ that have at least one node in the neighbour of $i$ $N(i)$, the gain of modularity $\Delta Q_{i \rightarrow c_j}$ that we have if we remove $i$ from its community $c_i$ and we assign it to $C_j$.
To due this, we can use the equations (\ref{ModularityC}) to calculate the modularity in current configuration $Q_{i\rightarrow c_i}$ and the modularity $Q_{i\rightarrow c_j}$ in the configuration where $i$ is assigned to $c_j$ and subtract, but this is quite inefficient. Instead, we can calculate directly  $\Delta Q_{i \rightarrow c_j}$:
\begin{equation}
\Delta Q_{i \rightarrow c_j} = \frac{l_{i\rightarrow c_j} - l_{i\rightarrow c_i / \{i\}}}{2|V|} + k_i \frac{k_{c_i / \{i\}} - k_{c_j}}{4|V|^2}
\end{equation}
where $l_{i\rightarrow c_j}$ is the sum of edges that connect $i$ to the community $c_j$, $k_i$ is the weight of the nodes $i$ and $k_{c_j}$ is the weight of the community $c_j$.
Then we define the subset $Z_i$ the set of community $c_z$ with $z \in N(i)$ such that:
\begin{align}
\Delta Q_{i \rightarrow c_z} \geq \Delta Q_{i \rightarrow c_j} && \forall j \in N(i)
\end{align}
If more there is more than one community in the group, one community $c_z^*$ was selected using a braking rule, otherwise we pick the only community in $Z_i$. If $\Delta Q_{i \rightarrow c_z^*} > 0$, we move the node $i$ to the community $c_z^*$. \\
This process is applied repeatedly and sequentially for all nodes while modularity score increases. When no more improvement can be reach, the second phase start. In this phase a new network was created from the results of the previous phase: in the new graph, the nodes are the communities found, and the edge between them are given by the sum of the links between nodes that belong to the corresponding communities (edge between nodes in the same communities lead to self-loop). Then we reapply the first step and then the second one until no more improvement is obtained. An example of the algorithm is shown in the Figure \ref{fig:blondelscheme}. \\
The complexity of this algorithm is $O(m)$ where $m$ is the number of the edges of the graph, due to the fact that we can compute the
gains in modularity for each neighbour easily. Respect to the previous approach, this techniques reaches the goal of the execution in linear times. Indeed, this algorithm can create an entire hierarchies of partitions and this can be useful to avoid the resolution limit problem: we can analyze in the dendrogram the intermediate solutions to observe its structure with the desired resolution \cite{Blondel_2008}.
\subsection{Pruning}
This algorithm even in the first formulation is quite efficient, but large network analysis requires improvement to be executed quickly. The parallel techniques are very useful for this task and it will be presented in the next chapter. Now we focus on a method that speed-up the computations in the sequential field but that is also suitable in parallel.\\
The first optimization phase is the most time consuming ones \cite{Blondel_2008}, consuming about 80\% of the time \cite{wickramaarachchi2014fast}. To reduce the impact of this first phase, in literature were proposed various approach. For example, in \cite{rand}, V. A. Traag proposed to randomize the choice of the community which you want to assign the nodes between the communities of the neighbour. The idea behind these techniques is that the nodes tend to be in a "good". This technique performs well sequentially if the graph were the community structure is well defined. Instead, in parallel behaviour, this method doesn't be so good due to the fact, each node changes communities simultaneously, and there is no way to prevent simultaneous swaps without introducing some overhead and this may lead to a convergence problem. For this reason, we choose another technique more parallel friendly, introduced by Ozaki et. al \cite{pruning}. Now we present this simple and efficient technique of optimization for this algorithm that doesn't afflict the quality of the partitions. This method makes a pruning of the nodes in the optimization phase in order to compute the maximum delta modularity for only the nodes that have the potential to change community.
Every time a node $i$ changes community from $X$ to $Y$, its affect the $\Delta Q$ of its neighbourhood and all nodes linked and in $X$ and $Y$. Referring to \ref{ModularityC}, we describe all these cases:
\begin{itemize}
	\item Nodes in $X$ that aren't connected to $i$:  for those nodes, the value of $\Delta Q_X$ increase because of the degree of the community $k_X$ decrease without affecting the value of $l_X$.
	\item Nodes in $Y$ that aren't connected to $i$: for those nodes, the value of $\Delta Q_Y$ decrease because the degree of the community $k_Y$ increase without affecting the value of $l_Y$.
	\item  Nodes that are linked to a node in $X$, but not to $i$: for those nodes, the value of $\Delta Q_X$ increase because of the degree of the community $k_X$ decrease without affecting the value of $l_X$.
	\item  Nodes that are linked to a node in $Y$, but not to $i$: for those nodes, the value of $\Delta Q_Y$ decrease because the degree of the community $k_Y$ increase without affecting the value of $l_Y$.
	\item Nodes that are linked to $i$ in $X$:
	in this case both $k_X$ and $l_X$ decrease for $\Delta Q_X$..
	\item Nodes that are linked to $i$ in $Y$:
	in this case both $k_Y$ and $l_Y$ increase for $\Delta Q_Y$.
	\item Nodes that are linked to $i$ but that are not either in $X$ or in $Y$:
	in that case afflict both $\Delta Q_X$ and $\Delta Q_Y$ (increase $k_X$, $l_X$, $k_Y$ and $l_Y$).
\end{itemize} 
The nodes considered in first and the fourth case, doesn't have the potential to change community: in the first case one increase the value of $\Delta Q_X$ that is the maximum (because they are already in the community $X$); in the fourth case one decrease the values of $\Delta Q_Y$ that aren't the maximum (because they are not in the community $Y$). In all other cases, there is a chance that some nodes change community. In the Figure \ref{fig:pruning}, the white nodes are the nodes that doesn't have the potential to change community, instead the black and striped ones are the ones that may have. Considering only this nodes, the computation time will be reduced without reducing the quality of the partition.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{0-resources/pruning}
	\caption{Example of graph where the nodes $i$ changed community from $X$ to $Y$. This image is reprinted from \cite{pruning}.}
	\label{fig:pruning}
\end{figure}\\
The optimization proposed by Ozaki et. al consists to create a set of nodes during the iteration of the optimization that will be analyzed in the next step: at the start of the optimization phase, an empty set $S$ is created and every time a node $i$ change its community, each nodes in its neighbourhood that doesn't belong to the new community is add to $S$. The next iteration consider only the nodes in $S$ and the process is iterated. They consider only one of the four previous categories of nodes: this is because calculating all nodes (explicitly the ones in the second and third group) introduce overhead and this group is the most influential for $\Delta Q$ \cite{pruning}. The selected nodes to be add to $S$ are the black one in the Figure \ref{fig:pruning}. 
The experimental result show that reduce the computational time by up to 90\% compared with the standard Louvain algorithm. In terms of accuracy, surprisingly, the modularity is almost the same,  not only the final one, but also the transition of the modularity during the iterations \cite{pruning}. 
\newpage
\subsection{Parallel Implementations}
Now we present various approach that was used in literature to improve the performance of the Louvain algorithm. We can divide the parallelization techniques in two different class: the coarse grained approach and the fine grained approach. The methods in the first class divides the nodes in some sets and the modularity are processed sequentially independently for each set. When all sets are analyzed, the algorithm merge the results for the next phase. Instead, the second approach consider each nodes independently. The best modularity were calculated for each node simultaneously, therefore the decision of the new community for each node is based on the previous configuration. Wickramaarachchi et al. \cite{wickramaarachchi2014fast} proposed one of the first coarse algorithm: in the first iteration, the algorithm partition the graph in subgraphs and the execution were performed simultaneously and independently on each partition. Edges that cross the partition were ignored. In terms of quality, they showed that ignoring edges cross partition edges does not impact to the quality
of the final result. \\
In 2015, both Staudt and Meyerhenke \cite{staudt2015engineering} and Lu et al. \cite{lu2015parallel} proposed an fine grained implementation based on OpenMP. To compute $Q_{i \rightarrow c_j}$ for each nodes $i$ and each communities $c_j$ in neighbourhood of $i$ , the algorithm  must calculate $l_{i\rightarrow c_j}$(i.e. the sum of edges that connect $i$ to the community $c_j$ ). These values may change in every new configuration: for this reason we must have a method to get it them fast. In \cite{staudt2015engineering}, they try to associate each node with a map in which the edge weight to neighbouring communities was stored and updated when node moves occurred, but they discover that introduced too much overhead.
Instead, recalculating each time the weight to neighbour communities each time a node is evaluated turned out to be faster. Therefore, they proposed to use a \verb|map| for each node as accumulator of his edges to calculate every $l_{i\rightarrow c_j}$. Instead the total weights of each community $k_{c_j}$ is stored and updated every time a nodes change community. The same scheme is used in \cite{lu2015parallel}. A more complex schema was proposed by Que et al. \cite{que2015scalable}: they proposed an algorithm based on a communication pattern that permit to propagate the community state of each nodes. 
Due to his complex behaviour, this schema is hard to implement on the GPU.\\
Forster in \cite{forster2016louvain} presented a GPU implementation based on the first two previous OpenMP version: he reports a speed-up to a factor of 12, but in the paper there isn't information about the quality of the partition. Following, the algorithm preposed form Naim et al. \cite{naim2017community} parallelize the hashing of the edges both in optimization and also in the aggregation phase. In addition, they partitioning the vertices into subsets on their degrees in order to obtain an even load balance between threads.
A different implementation was proposed by Cheong et al. \cite{cheong2013hierarchical}: it is a multi-GPUs implementation that used a coarse grain model between the GPUs and than a fine grain model for the computation of the modularity of each sub-graphs. This algorithm its also peculiar because doesn't use hashing to calculate the modularity but sort each neighbour list
based on the community ID of each neighbouring vertex.
