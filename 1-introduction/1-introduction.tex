\section{Introduction}
The community detection problem is one of the most interesting fields of graph analysis. In several real-world scenarios, we have the necessity of cluster some data considering the relations between them: one example can be the problem of dividing social network users in groups by the mutual friendship relations to propose targeted advertisings. A natural way to represent this kind of structure based on the relation is a graph, and several techniques based on this theory was proposed in the literature to solve this kind of problem. This problem is not easy to solve due to the extremely high number of different possible partitions of the data that we can perform, even if the dataset is composed of few elements.
One the most famous and used heuristic is the Louvain algorithm proposed by Blondel et al. \cite{Blondel_2008}, due to its speed and its overall quality, even if its limits are well known in the literature. This greedy algorithm divides the graph into partitions maximizing a quality function that evaluate them, called \textbf{modularity}. This function is based on the idea that a random graph doesn't exhibit a community structure: therefore, we can evaluate how is well defined the community structure comparing the graph with another graph that keeps some of its structural proprieties but is generated at random, called \textit{null-model} \cite{Girvan2002Community}. Given a graph $G(V,E)$, its adjacency matrix $A$, a partition of nodes $C$ and the corresponding function $c(x)$ that assign each node $x$ to its community, we define the modularity function as follow:
\begin{equation}\label{intro_Q}
Q = \frac{1}{2|E|} \sum_{i,j \in V}\left(A_{ij} - \frac{k_ik_j}{2|E|}\right) \delta(c(i), c(j))
\end{equation}
where $k_i$ is the degree of the node $i$ and $\delta$ is an filter function: its yields one if $c(i) = c(j)$, zero otherwise.
The techniques proposed before the Louvain algorithm are quite slow because, every time we assign a node to a community, we have to recalculate the values with the previous formula, and this calculation takes a lot of time. To speed up the computation, this algorithm proposed to calculate the variation in modularity assigning a node to another community without recalculating $Q$ with the Formula \ref{intro_Q}. The algorithm is composed of two phases: an optimization phase and an aggregation phase. At the beginning each node is assigned to a community composed only by itself; in the first, we pick each node $i$ of the graph and we calculate for each community $c_j$ in its neighbourhood the values $\Delta Q_{i \rightarrow c_j}$, that is the changing in the modularity of assigning the node $i$ to the community $c_j$, as following:
\begin{equation}\label{intro_Delta}
\Delta Q_{i \rightarrow c_j} = \frac{l_{i\rightarrow c_j} - l_{i\rightarrow c_i / \{i\}}}{2|V|} + k_i \frac{k_{c_i / \{i\}} - k_{c_j}}{4|V|^2}
\end{equation}
where $l_{i\rightarrow c_j}$ is the sum of edges that connect $i$ to the community $c_j$, $k_i$ is the weight of the nodes $i$ and $k_{c_j}$ is the weight of the community $c_j$. Then, for each node, we select the maximum values and if it is greater than zero, we assign the node $i$ to the community that corresponds to the maximum value. We repeat this procedure sequentially for all nodes while the modularity score increases. When no more improvement can be achieved, we execute the aggregation phase. In this phases we create a new graph from the current community structure: each node is one of this communities, and the edge between them is given by the sum of the links between nodes that belong to the corresponding communities (edge between nodes in the same communities lead to self-loop). After this step, we reapply the first step. The algorithm ends up when no more improvement is obtained.
This algorithm is quite efficient with a complexity of $O(m)$ where $m$ is the number of edges in the graph, but this algorithm requires a lot of time to find the communities in the bigger graphs. For this reason, some approaches were proposed in the literature to speed up the algorithm. One interesting technique proposed by Ozaki et. al \cite{pruning}, prune the nodes in the optimization phase, considering only the nodes that have a neighbour that has to change its community in the previous iteration. Apart from this kind technique, generally, the most effective method to improve significantly the performance of an algorithm is to parallelize the execution. Especially, to obtain the maximum speed, many frameworks that allow performing on the GPU computation normally handled by the CPU became popular. The most used framework is the Nvidia CUDA, a parallel computing platform and application programming interface. CUDA was originally designed as a C++ language extension that allows any developer to building application designed for the GPU with a low learning curve. CUDA is also designed to transparently scale on different GPU.\\
In literature, several implementations of the Louvain algorithm for the GPU were proposed. In this thesis, we present three new algorithms for the GPU based on the Louvain algorithm: PSR-Louvain, PH-Louvain and Adaptive Louvain. All these algorithms implement the pruning techniques proposed by Ozaki. No other Louvain based algorithm for the GPU implements this technique before. 
These algorithms compute the maximum score of $\Delta Q$ for each node simultaneously, and then we update the community based on that score. Also, the creation of the edges of the new graph in the aggregation phase is executed in parallel. 
The first two algorithms are quite similar to each other. In both algorithm we start from the edge list composed by the tuple $(i,j,w)$ where $i$ is the source node of the edge, $j$ is the destination node of the edge and $w$ is the weight of the edge.
Both algorithm, in the optimization phase, first select only the edges such that the node $i$ has a neighbour that has to change its community in the previous iteration, then they substitute each destination node with the corresponding community $c_j$. After them, the two algorithms using different methods to sum up all the values $w$ for each pair $i$ and $c_j$ and obtain the corresponding $l_{i\rightarrow c_j}$. We need this value to compute all the $\Delta Q_{i \rightarrow c_j}$, as shown in the Formula \ref{intro_Delta}. The first algorithm sorts the list and then performs a segmented reduction to each value with the same pair $i$ and $c_j$; the second algorithm use a hashmap to aggregate all of these values. After then, both select the maximum value for each node and eventually update the community. Finally, they compute if a node has a neighbour that change its community and store the results that will be used in the next iteration for the pruning. These two algorithms use the same aggregation scheme proposed in the optimization phase to create the new graph in the aggregation phase. In both algorithm is also present a technique that optimizes the first iteration of each optimization phase. \\
This two algorithm, compared to the fastest sequential versions, obtain a speed-up factor up to 56.
During the analysis of this two algorithm, we discover that the PSR-Louvain tends to perform better than the PH-Louvain in the early iteration of the optimization phase; on the other hand, the PH-Louvain approach outperforms the other when the number of a different key inserted in the hashmap falls below a given threshold. Besides the PH-Louvain aggregation phase is faster than the PSR-Louvain one. \\
On these considerations, we create a new algorithm that combines these two algorithms and selects the best aggregation scheme on the situation. This Adaptive approach performs better than the other, combining effectively the best feature of these two algorithms. Finally, we compare this version with the two fastest GPU Louvain based algorithm: the first one is included in the Nvidia library cuGraph; the second one is included in the high-performance library for graph analysis Gunrock. Our test highlight that our Adaptive algorithm optimizes the memory occupation better than the other two allowing it to compute graph with approximately twice edges. Besides, our algorithm generally perform better than these two algorithms in terms of time.\\
This thesis is structured as follows:
in Chapter \ref{C2} of this thesis, we present the Nvidia GPUs architecture and the CUDA framework, paying special attention to the concept that is used later on this thesis; in the Chapter \ref{C3} we present the field of the community detection and the modularity optimization, highlighting the motivation that leads to the design of the Louvain Algorithm; in the Chapter \ref{C4} we present the sequential Louvain algorithm, the pruning techniques and the previous parallel implementations presented in the literature;
then, in Chapter \ref{GPUalg}, we present in details our two first algorithm, that will be analyzed in details in the Chapter \ref{C6}; finally, in the Chapter \ref{C7}, we present the Adaptive Louvain algorithm and we analyze it, comparing it first with our other two algorithms, then with the two algorithms included in the two libraries. In the last chapter, we sum up our work and we highlight some future research areas and development.