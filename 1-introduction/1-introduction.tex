\section{Introduction}
The community detection problem is one of the most interesting field of the graph analysis. In several real world scenarios, we have the necessity of cluster some data considering the relations between them: one example can be the problem of dividing social network users in group by the mutual friendship relation in order to propose targeted advertisings. A natural way to represent this kind of structure based on the relation is a graph, and several technique based on this theory was proposed in the literature in order to solve this kind of problem. Actually, this problem is not easy to solve due to the extremely high number of different possible partition of the data that we can perform, even if the dataset is composed by just few elements.
One the most famous and used heuristic is the Louvain algorithm proposed by Blondel et al. \cite{Blondel_2008}, due to its speed and its overall quality, even if some limits of it are well known in the literature. This greedy algorithm divides the graph in partitions that maximize a quality function that evaluate this partitions, called modularity. This function is based on the idea that a random graph doesn't exhibit a community structure: therefore, we can evaluate how is well defined the community structure comparing the graph with another graph that keep some structural proprieties but is generated at random, called \textit{null-model} \cite{Girvan2002Community}. Given a graph $G(V,E)$, its adjacency matrix $A$, a partition of nodes $C$ and the corresponding function $c(x)$ that assign each node $x$ to its community, we define the modularity function as follow:
\begin{equation}\label{intro_Q}
Q = \frac{1}{2|E|} \sum_{i,j \in V}\left(A_{ij} - \frac{k_ik_j}{2|E|}\right) \delta(c(i), c(j))
\end{equation}
where $k_i$ is the degree of the node $i$ and $\delta$ is an filter function: its yields one if $c(i) = c(j)$, zero otherwise.
The techniques proposed before the Louvain algorithm are quite slow due to the fact that, every time we assign a node to a community, we have to recalculate the values with the previous formula, and this calculation takes a lot of time. To speed up the computation, this algorithm proposed to calculate the variation in modularity assigning a node to another community without recalculate $Q$ with the formula \ref{intro_Q}. The algorithm is composed by two phases: an optimization phase and a aggregation phase. At the beginning each node is assigned to a community composed only by itself; in the first we pick each node $i$ of the graph and we calculate for each community $c_j$ in its neighbourhood the values $\Delta Q_{i \rightarrow c_j}$, that is the changing in modularity of assigning the node $i$ to the community $c_j$, as following:
\begin{equation}\label{intro_Delta}
\Delta Q_{i \rightarrow c_j} = \frac{l_{i\rightarrow c_j} - l_{i\rightarrow c_i / \{i\}}}{2|V|} + k_i \frac{k_{c_i / \{i\}} - k_{c_j}}{4|V|^2}
\end{equation}
where $l_{i\rightarrow c_j}$ is the sum of edges that connect $i$ to the community $c_j$, $k_i$ is the weight of the nodes $i$ and $k_{c_j}$ is the weight of the community $c_j$. Then, for each node, we select the maximum values and if it is greater than zero, we assign the node $i$ to the community that correspond to the maximum value. We repeat this procedure sequentially for all nodes while modularity score increases. When no more improvement can be achieved, we execute the aggregation phase. In this phases we create a new graph from the current community structure: each node is one of this communities, and the edge between them are given by the sum of the links between nodes that belong to the corresponding communities (edge between nodes in the same communities lead to self-loop). After this step we reapply the first step. The algorithm ends up when no more improvement is obtained.
This algorithm is quite efficient with a complexity of $O(m)$ where $m$ is the number of edges in the graph, but, nevertheless, this algorithm requires a lot of time to find the communities in the bigger graphs. For this reason, some approach were proposed in the literature to speed up the algorithm. One interesting techniques proposed by Ozaki et. al \cite{pruning}, prune the nodes in the optimization phase, considering only the nodes that have a neighbour that has change its community in the previous iteration. Apart of this kind technique, generally the most effective method to improve significantly the performance of an algorithm is to parallelize the execution. Especially, to obtain the maximum speed, many framework that allow to perform on the GPU computation normally handled by the CPU became popular. The most used framework is the Nvidia CUDA, a parallel computing platform and application programming interface model. CUDA was originally designed as a C++ language extension that allow any developer to building application designed for the GPU with a low learning curve. CUDA is also designed to transparently scale on different GPU.\\
In literature, few implementation of the Louvain algorithm for the GPU were proposed. In this thesis, we present three new algorithm for the GPU based on the Louvain algorithm: PSR-Louvain, PH-Louvain and Adaptive Louvain. All these algorithm implements the pruning techniques proposed by Ozaki. No other Louvain based algorithm for the GPU implements this technique before. 
These algorithm compute the maximum score $\Delta Q$ for each node simultaneously, and then we update the community based on that score. Also the creation of the edges of the new graph in the aggregation phase are executed in parallel. 
The first two algorithm are quite similar each other. In both algorithm we start from the edge list composed by the tuple $(i,j,w)$ where $i$ is the source node of the edge, $j$ is the destination node of the edge and $w$ is the weight of the edge.
Both algorithm, in the optimization phase, first select only the edges such that the node $i$ have a neighbour that has change its community in the previous iteration, than they substitute each destination node with the corresponding community $c_j$. After them, the two algorithm using different method to sum up all the values $w$ for each pair $i$ and $c_j$ and obtain the corresponding $l_{i\rightarrow c_j}$. We need this value to compute all the $\Delta Q_{i \rightarrow c_j}$, as shown in the Formula \ref{intro_Delta}. The first algorithm sorts the list and then performs a segmented reduction to each value with the same pair $i$ and $c_j$; the second algorithm use an hashmap to aggregate all of this values. After than, both select the maximum value for each node and eventually update the community. Finally, they compute if a node has a neighbour that change its community and store the results that will be used in the next iteration for the pruning. These two algorithm use the same aggregation scheme proposed in the optimization phase to create the new graph in the aggregation phase. In both algorithm is also present an technique that optimize the first iteration of each optimization phase. \\
This two algorithm, compared to the fastest sequential versions, obtain a speed up factor up to 56.
During the analysis of this two algorithm, we discover that the PSR-Louvain tends to perform better than the PH-Louvain in the early iteration of the optimization phase; on the other hand, the PH-Louvain approach outperforms the other when the number of different key inserted in the hashmap falls below a given threshold. Besides the PH-Louvain aggregation phase is faster than the PSR-Louvain one. \\
On this two consideration, we create a new algorithm that combine this two algorithm, and select the best aggregation scheme on the situation. This Adaptive approach performs better than the other, combining the best feature of this two algorithm. Finally we compare this version with the two fastest GPU Louvain based algorithm: the first one is included in the Nvidia library cuGraph; the second one is included in the high performance library for graph analysis Gunrock. Our test highlight that our Adaptive algorithm optimize the memory occupation better than the other two allowing it to compute graph with approximately twice edges. Besides, our algorithm generally perform better than this two algorithm in terms of time.\\
This thesis is structured as following:
in the Chapter \ref{C2} of this thesis we present the Nvidia GPUs architecture and the CUDA framework, paying special attention on the concept that are used later on this thesis; in the Chapter \ref{C3} we present the field of the community detection and the modularity optimization, highlighting the motivation that leads to the design of the Louvain Algorithm; in the Chapter \ref{C4} we present the sequential Louvain algorithm, the pruning techniques and the previous parallel implementations presented in the literature;
then, in Chapter \ref{GPUalg}, we present in details out two first algorithm, that will be analyzed in details in the Chapter \ref{C6}; finally, in the Chapter \ref{C7}, we present the Adaptive Louvain Algorithm and we analyze it, comparing it before with our other two algorithms, then with the two algorithm included in the two libraries. In the last chapter, we sum up our work and we highlight some future research areas and development. 