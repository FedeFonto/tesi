\section{Conclusion and Future Work}\label{c8}
Summing up, in the first half of this thesis we explain the community detection problem and the Louvain algorithm and why the parallelization of this algorithm but also, in general, the High-Performance Computing are interesting fields nowadays. After this brief introduction, we proposed our algorithms.
In conclusion, the objectives of this thesis are two: the first is to propose and analyze the effectiveness of a pruning approach in a GPU parallel Louvain algorithm for the first time; the second is to analyze the different approaches to aggregate the edges used in literature, i.e. the sort-reduce approach versus the hashmap, in order to find the pro and cons of each one. \\
In this thesis, we demonstrate that introducing a pruning approach in a GPU algorithm based on the Louvain is possible and we have a variable improvement that depends on how many nodes were pruned during the iteration. If we prune a lot of nodes, we have a  significant improvement with respect to a similar version without the pruning approach enable. In our algorithms, we store all the variables on the GPU and every parallel computation are executed on it. As a consequence, we are limited to the size of the GPU memory, that allows us to analyze graph up to \\$\sim$ 175 000 000 edges. As future work, we left the creation of a hybrid CPU-GPU algorithm that can analyze graph that exceeds the GPU memory, permitting to transfer part of the edges to analyze in the GPU from the CPU only for the computations. This kind of algorithm may have advantages by using the pruning techniques: therefore one of the biggest bottlenecks of this kind of algorithm is the time needed to transfer the data from the host memory to the device memory; reducing the number of data transferred in the GPU may lead an improvement in terms of times. A similar observation can be made for the design of a Multi GPUs algorithm. \\
We also analyze the differences between two different approaches to aggregate the edges in order to calculate how many edges links a node with each community in its neighbourhood. The first approach sort a list of tuples (node, community, weight) and perform a segmented reduction using the pair (node, community) as key. The second approach inserts these tuples in a map, using as a key the pair (node, community) and sum the weight to the value in the map, if it is already present. Our tests discover that the two approaches perform differently according to the situation: the first one performs better in the first iterations of the optimization phases; instead, the second approach performs better if the number of different keys inserted on the total possible key drops below a threshold. We notice also that the aggregation phase based on the map tends to performs better than the other, thanks to the reduction in the number of community. We left as future work a more in depth analysis of when one approaches performs better than the other in this phase. Therefore, we create an adaptive version of the algorithm that chooses the best technique to aggregate based on the situation. Our test shows that this technique performs better than the original two, with a maximum improvements of $\sim 17\%$. Besides, we evaluate the performance of this algorithm with two GPU algorithms presented in two important libraries: the first one is included in the cuGraph library, part of the Nvidia project RAPIDS; the second is the Gunrock library, whose its research team is one of the main contributors of the cuGraph library. We discover that our algorithm scales better than the other two, thanks to a technique that allows us to process the optimization phase on buckets of a given size, reducing the consumption of memory in that phase. Our tests show that we can analyze graph with almost twice the edges compared to the other algorithms. This technique could be an important starting point to 
define an algorithm that can analyze a graph that doesn't fit in the GPU memory.
We also discover that our algorithm performs slightly better of this two on the biggest graph in our test, with a mean improvement of the $\sim 13\%$ on our six biggest datasets. This tests open also two new interesting research area. The cuGraph Louvain algorithm uses a hashmap for each node instead of a global one like in our Adaptive Louvain algorithm: we left as future work the analysis of the performance of this aggregation technique compared to the global hashmap that we present in our thesis. Besides, the Gunrock Louvain algorithm doesn't perform the modularity recalculation correctly after each iteration of the optimization phase and has a limit on the different number of iteration that can perform in one phase. Despite this behaviour, that, theoretically, can in some case even lead to not reach the local maximum, this algorithm performs well and find a good score of modularity. We left as future work the analysis of this particular behaviour, paying special attention to how avoiding some operation, could have an impact on the overall performance, both in terms of modularity and in terms of times.\\
Ultimately, the most important contribution of this thesis is that lay the foundation for the design of a new Louvain algorithm for the GPU that can allow us to analyze graphs with billion edges quickly. We have quantified the impact of the pruning approach in the parallel algorithms, and this can be extremely useful for an algorithm that can handle graphs that don't fit in memory. We highlight also the pro and cons of the various approaches used in literature to compute the score $\Delta Q$, suggesting an adaptive approach to compute quickly the partitions.