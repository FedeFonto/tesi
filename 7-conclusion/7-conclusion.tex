\section{Conclusion and Future Work}\label{c8}
Summing up, in the first half of this thesis we explain the community detection problem and the Louvain algorithm and why the parallelization of this algorithm but also, in general, the High-Performance Computing are interesting fields nowadays. After this brief introduction, we proposed our algorithms.
In conclusion, the objectives of this thesis are two: the first is to propose and analyze the effectiveness of a pruning approach in a GPU parallel Louvain algorithm for the first time; the second is to analyze the different approaches to aggregate the edges use in literature, i.e. the sort-reduce approach versus the hashmap, in order to find the pro and cons of each one. \\
In this thesis, we demonstrate that introducing a pruning approach in a GPU algorithm based on the Louvain is possible and we have a variable improvement that depends on how many nodes were pruned during the iteration. If we prune a lot of nodes, we have a  significant improvement with respect to a similar version without the pruning approach enable. In our algorithms, we store all the variables on the GPU and every parallel computation are executed on it. As a consequence, we are limited to the size of the GPU memory, that allows us to analyze graph up to \\$\sim$ 175 000 000. As future work, we left the creation of a hybrid CPU-GPU algorithm that can analyze graph that exceeds the GPU memory, permitting to transfer part of the edges to analyze in the GPU from the CPU only for the computations. This kind of algorithm may be placed an advantage by the using of the pruning techniques: therefore one of the biggest bottlenecks of this kind of algorithm is the time necessary to transfer the data from the host memory to the device memory; reducing the number of data transferred in the GPU may lead an improvement in terms of times. A similar observation can be made for the design of a Multi GPUs algorithm. \\
We also analyze the differences between two different approaches to aggregate the edges in order to calculate how many edges links a node with each community in its neighbourhood. The first approach sort a list of tuples (node, community, weight) and perform a segmented reduction using the pair (node, community) as key. The second approach inserts these tuples in a map, using as a key the pair (node, community) and sum the weight to the value in the map if it is already present. Our tests discover that the two approaches performs differently based on the situation: the first one perform better in the first iteration of the optimization phases; instead, the second approach performs better if the number of different keys inserted on the total possible key drops below a threshold. We notice also that the aggregation phase based on the map performs better than the other. Therefore, we create an adaptive version of the algorithm that chooses the best technique to aggregate based on the situation. Our test shows that this technique performs better than the original two. Besides, we evaluate the performance of this algorithm with two GPU algorithms presented in two important libraries: the first one is included in the cuGraph library, part of the Nvidia project RAPIDS; the second is the Gunrock library, whose its research team is one of the main contributors of the cuGraph library. We discover that our algorithm scales better principally thanks to a technique that allows us to process the optimization phase on buckets of a given size, reducing the consumption of memory in that phase. We also discover that our algorithm performs slightly better of this two on the biggest graph in our test. This tests open also two new interesting research area. The cuGraph Louvain algorithm uses a hashmap for each node instead of a global one like in our Adaptive Louvain algorithm: we left as future work the analysis of the performance of this aggregation technique compared to the global hashmap that we present in our thesis. Besides, the Gunrock Louvain algorithm doesn't perform the modularity recalculation correctly after each iteration of the optimization phase and has a limit on the different number of iteration that can perform in one phase. Despite this behaviour, that theoretically can in some case even lead to not reach the local maximum, this algorithm performs well and find a good score of modularity. Then, we left as future work the analysis of this particular behaviour, paying special attention to how avoiding some operation have an impact on the overall performance both in terms of modularity and in terms of time. 