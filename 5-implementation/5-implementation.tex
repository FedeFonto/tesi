\section{The GPU's Algorithms}
In this chapter, we present two novel parallel implementations of the Louvain algorithm: both versions implement the pruning presented by Ozaki et. al. \cite{pruning}. The first one is based on the sort-reduce pattern:
to accumulate the edges to calculate $l_{i\rightarrow C_j}$  (see formula \ref{ModularityC}),  it sorts the list and performs a reduction of consecutive values with the same key.  We also use a reduction on a sorted array to compute the maximum values of modularity for each node. 
The second algorithm uses a map to accumulate.
In this chapter, we present firstly the the algorithms, then a special speed-up technique of the first iteration of the optimization phase included in both algorithms and finally the data structure and the implementations.

\subsection{Prune-Sort-Reduce}\label{Prune-Sort-Reduce}
The Prune-Sort-Reduce Louvain algorithm is the first version of the algorithm that we present in this thesis. This algorithm implements firstly the pruning presented by Ozaki in the parallel behaviour \cite{pruning}. This algorithm uses a scheme of computation inspired by \cite{cheong2013hierarchical}: we create a list of pair $(nodes, community)$, we sort it and we aggregate the values using the node id as the key of a reduction. In the beginning, we have each node in its self-community. As the sequential Louvain algorithm, we can divide it into two steps iterated alternately: the optimization phase and the aggregation phase. 
At its turn, we divide the optimization phase in eight sub-phases, in which the operations are executed in parallel. The sub-phases from the first to sixth are executed repeatedly on some fixed part of the edges until all the edges are considered, in order to reduce the memory used simultaneously. In order to compute the right maximum values of $\Delta Q$ for each node, we split the edges into buckets such that if there is an edge with source node $n$ in the bucket, also all the other edges that belong to $n$ are included. To perform this, we use the \verb|neighbourhood_sum| vector to select the right range. We store the results at the end of the sixth sub-phases. When the algorithm compute the maximum $\Delta Q$ for each node that match the pruning criteria, we start the last two sub-phases. For the sake of simplicity, we present the case in which all edges are in exactly one bucket and all the phases are executed one after another. The phases are the following:
\begin{enumerate}
	\item \textbf{Copy sub-phase:} in the first step, we copy from the graph all the edges of the nodes that we consider in this iteration. To due this, we check on a support vector if the source nodes of the edges have a neighbour that has change community according to the criteria presented in Chapter \ref{prun}. The support vector contains boolean, it has length $n$ and in position $i$ there is a True if the nodes $i$ matched the criteria in the previous iteration, False otherwise. This vector was filled in the last phases; at the first iteration, there are only True values in it. We exclude also the self-loops because we don't consider them in the computation of the various values of $\Delta Q$. Besides, in this phase, we transform the vector of destination nodes in a vector with the associated community. Therefore, we obtain three vectors that contain the source node, the community of destination and the weight of the edges.  
	\item \textbf{Sort sub-phase:} in the second phase, we sort the vectors obtained in the first phase. We sort it using the tuple $(node, community)$ as key and the weight as value.
	To do this, we use a  \verb|thrust::zip_iterator| that allow us to consider this two vectors as if they are a unique vector of pair.  At the end of this phase, we have that all the tuples $(node, community, weight)$ with the same $(node, community)$ are consecutive. Besides, all the tuples with the same source node are consecutive.
	\item \textbf{Reduce sub-phase:} in this step we perform a reduction by key, i.e. we sum up all consecutive values with the same key. We use as key the tuple $(node, community)$: doing this, we obtain a unique tuple $(node, community,$  $tot\_weight)$ for each pair $(node, community)$. After this operation, in the weights value we have the values $l_{i\rightarrow c_j}$ related to the nodes $i$ and the community $c_j$.
	\item \textbf{Self-counting sub-phase:} In this phase, we isolate all the tuple  $(i, c, w)$ such that 
	$c$ is the actual community for the nodes $i$. We need to isolate these values because we use it in the next phase to compute the various values of $\Delta Q$.
	\item \textbf{Compute Delta sub-phase:} In this phase, given each tuple $(i, c, w)$, using the formula \ref{delta_q}, we substitute each values in the weights array with the associated $\Delta Q_{i\rightarrow c}$.
	\item \textbf{Select Max sub-phase:} Now we need to isolate the maximum $\Delta Q_{n\rightarrow c}$ for each node $n$. Considering that the vectors are already sorted, we can perform this operation using another reduction but, in this case, we return only the maximum weight instead of performing the sum. We execute this operation using the nodes vector as keys, and the other two as value, but we considering the communities only if the values $\Delta Q$ are equals. In that case, we use the minimum labelling heuristic (Chapter \ref{parallel-imp}) as braking rule.  After this step, we have exactly one tuple  $(i, c, \Delta Q_{n\rightarrow c})$  for each nodes $i$.
	\item \textbf{Update Community sub-phase:}\label{update_com} In this step, we update the community for each node if the values of $\Delta Q$ is greater than 0. We also update the related community weights: we remove the weight of the node from the previous community and we add it to the new one. These operations are atomic to avoid concurrent operations. We also implement the minimum labelling heuristic to prevent the swap of singleton nodes presented in the Chapter \ref{parallel-imp}. Besides, in this phase, we keep on a support vector of length $n$ if the nodes change communities in this iteration. This vector is different respect to the array that we use in the first step: this one store keeps track if a node change communities, the other if a node has a neighbour that change its community. In the following step, we create the second vector from the first one.
	\item \textbf{Update Pruning sub-phase:}\label{update_prun}  To update the vector that handles the pruning criteria, we firstly set all its elements to zero. After that, a method takes each edge of the graph and check if the destination edge has changed its communities in the previous iteration: if this happened, it set the corresponding node's value to True. This update operation is not atomic, because multiple threads can set only a True to the same position and there aren't conflict.
\end{enumerate}
After then we compute the new total modularity and we subtract the old value to new one: if the value is upper than a given threshold, we will repeat these steps, otherwise we start the aggregation phase. We highlight that we can not add directly the various $\Delta Q$ obtained in the optimization step to the old modularity
like the sequential algorithm, because all nodes change communities simultaneously and consequently this value is not reliable any more.\\
The aggregation phase use several similar concepts presented previously, and we can divide it into four sub-phases in which the operations are executed in parallel:
\begin{enumerate}
	\item \textbf{Re-indexing communities sub-phase:}\label{Re-indexing} in the first phase, before the graph contraction, we renumber the community. Actually, we have only certain communities associated to the nodes respect to the initial configuration: for example, if a nodes $i$ change community from $c_i$ to $c_j$ in the first iteration of the optimization phase, no nodes are assigned to $c_j$ after the update and no nodes can select the communities $c_j$ from that moment. This cause a useless waste of memory if we continue to keep all those unused values in the community weight. For this reason, we need to create a map to rearrange the communities index.
	First, we create a support vector such that at the position $c$ there is a 1 if the community $c$ has a weight greater than 0 (i.e. there is at least one node assigned to this community). Then we perform a prefix sum on this vector: in this way at the position $c$ there is the new index incremented by one for the community $c$ (please note: incremented by one because we counting from zero!). We make an example to clarify this step. We have the communities weight vector: $[0,3,0,1,0,0,4]$. We create the support vector inserting one if the weights are greater than 0 and we obtain: $[0,1,0,1,0,0,1]$. Executing the prefix sum on this vector, we obtain: $[0,1,1,2,2,2,3]$. To calculate the new index of the community $3$, for example, we have to go at the position $3$ in the vector and decrease the value by one, obtaining $1$. The values in position $0,2,4,5$, even if has a value in this vector (that in some case can be repeated), we ignore it because no nodes are in those communities, therefore we haven't index conflict. When this renumbering map is ready, we start the next phase.
	\item \textbf{Transform edges sub-phase:} In this step, all the pairs of edges $(i, j)$ of the original graph are transformed in the pair $(c_i, c_j)$ where $c_i$ is the community associated to $i$. In this phase, we also apply the map to renumber the communities that we create in the previous step. 
	\item \textbf{Sort-Reduce sub-phase:} In this phase we sort all the edges $(c_i, c_j, w)$ using as a key for the sorting the pair $(c_i, c_j)$. After this, we reduce the edges vector still using as a key $(c_i, c_j)$. After this step we have contract the graph summing up all the edges that lay between two communities.
	\item \textbf{Update variables sub-phase:}\label{updategraph}  In the last step, we update all the support value in the graph object and the community object: in particular in the first object we recalculate \verb|tot_weight_per_nodes|, \verb|neighbourhood_sum|, \verb|n_nodes| and \verb|n_links|; in the second object we assign each node to its self community and we set the communities weights consequently.
\end{enumerate}
The algorithm continues until we can not have further improvement in the modularity update. In this version of the algorithm, we keep only the best result to not occupy several device memory, but it is possible trivially save the intermediate result adding a step that copies the clustering results on the host memory after the re-indexing sub-phase (in this way we have consistent indexing among the dendrogram).
\subsection{Hashmap Version}
The second version of the parallel Louvain algorithm uses a different approach to aggregate the edges: we use a special global hashmap instead of sorted vectors.
Using a map to accumulate some values by its key is a standard approach to solve this problem because the map allows to retrieve and insert an object in $O(1)$ time. 
To obtain this performance, the hashmap uses a function named hash function to dispose at random the objects in the memory. This creates a problem on the GPU because uncoalesced memory accesses is an order of magnitude slower than sequential memory accesses. 
To overcome this problem this map uses a system of open-addressing based on the cuckoo hashing: this type of map is the one that performs better on the GPUs \cite{alcantara2012building}. This map is stored in the device global memory and uses 64 bits for the keys and 32 bits for the value. We choose to use 64 bits for the key because we need to store a pair of 32 bits keys (the pair $(node, community)$ in the optimization phase and the $(community, community)$ pair in the aggregation phase). This map has $r$ different hash function, each one associated to an id $r_i$ where $i \in [0, r-1]$. When we insert a new pair key-value $(k,v)$, we use the hash function with id $r_0$ to compute the position of the new key $v$: if the slot is empty, we add the key and the value, we use the next function $r_1$ otherwise. We continue to search a empty slot following this schema: if $r_i$ is not empty, we retry to insert the pair with function $r_{i+1}$. If all the function fails to insert the new pair, we raise an error. The main difference between this map and the classic cuckoo hashing is that we don't "kick out" the old key when we have a conflict in order to find a new memory address for it, but we hash with a different function the pair that we have to insert: this because to make a classic cuckoo hashing, we need a set of atomic operations for 128bits to do the "kick out" operation in parallel without generating race condition. This type of atomic operation in CUDA can be done only on variable up to 64 bits. Besides, this map has another special feature: if we insert a pair $(k,v)$ and there is already an entry in the table with a key $k$, the map automatically sum the values $v$ to the one stored in the map. Indeed, we use this map only to aggregate values, and for this reason, we design it to this operation as fast as possible. The last feature that we add to this table is the contract table operation: these methods re-arrange the vectors used to store the pairs in order to allow us to accessing them sequentially. After this operation, we can not get the entry using the hash function, because the memory is re-organized. We use this operation before the computation of $\Delta Q$ to increase the performance and still working considering the edges $(nodes, communities)$ independently. We describe this step later in this chapter. This contract table operation is made quickly using the function \verb|thrust::remove_if| that remove from the vector every element $x$ that satisfies a predicate and then contract the vector. The predicate that we use in this method check if the memory slot in the vector is empty. \\
Now that we have presented the map, we present the algorithm. Also, this algorithm implements the pruning operation heuristic. We can divide the optimization phase into six sub-phases.
Like the previous algorithm, we execute the first four on a fixed part of the edges to reduce the memory consumption and in these buckets, all the edges belong to a certain set of nodes, to compute the right values. When all the edges are analyzed and the results are stored, we execute the last two sub-phases. As previous, we present the case in which all edges are in exactly one bucket and all the phases are executed one after another:
\begin{enumerate}
	\item \textbf{Fill Map sub-phase:} in the first step, we fill the map with all tuple \\$(node, communities, weight)$ that has the source node that matching the criteria presented in Chapter \ref{prun}. We use the same array presented in the previous algorithm to perform this operation. We also exclude all the self-loops in this phase. At the end of this step we obtain a map where every non-empty entry has the form $(i, c_i, l_{i \rightarrow c_j})$ \ref{delta_q}. The map also isolate in a different vector each values $l_{i \rightarrow c_i}$ such that $c_i$ is the community of the nodes $i$ is stored at position $i$. This operation was made because in the delta step, after the contraction, we can not get any more the position of the entry using the hash function, and we need a method to get the nodes communities weights quickly.
	\item \textbf{Contract Table sub-phase:} in the first step, using a map to calculate $l_{i \rightarrow c_j}$ allows accessing to the memory address in $O(1)$ time for each edge. But to compute the relative delta and the maximum, we have to access the memory using an uncoalesced memory access pattern. In addition, we have no idea of how and which community are in the neighbourhood of a given node: many application allocates a thread for each edge, transform the edge destination from nodes to community and then check the maximum. This approach lay to check multiple time the score of the community $c$ if two neighbours of the nodes $n$ are in $c$. To overcome this problem, we contract the table: the vectors of the table are re-organized in order to permit sequential access, without empty slot between the entries as explained previously. After this operation, the map became a pair of vectors: at each position of the first one there is the key node-community $(i, c_j)$, in the second one the relative $l_{i \rightarrow c_j}$. The support vector with the sum of edges of the node that connects it with another in the same community is not re-arranged by this operation.
	\item \textbf{Compute Delta sub-phase:} now we can compute the $\Delta Q_{i\rightarrow c_j}$ for each tuple $(i, c_j, l_{i \rightarrow c_j})$ using the formula \ref{delta_q}. The result overwrites the last value in the tuple (we doesn't need that value any more). To get the sum of edges that connect the nodes to its actual community, we use the vector created in the first step.
	\item \textbf{Select Max sub-phase:} now we have two vector in which are stored the tuple $(i, c_j, \Delta Q_{i\rightarrow c_j})$. Now we have to select the pair $(i, \Delta Q_{i\rightarrow c_j})$ for each node $i$ such that $\Delta Q_{i\rightarrow c_j}$ is maximum. We can not use as the previous algorithm a reduce operation because we haven't a sorted array and the sorting operation is too expansive. Instead, we declare two support array of length $n$ where $n$ is the number of nodes. Then, for each tuple, we use the \verb|atomicMax| operation to check if the value $\Delta Q_{i\rightarrow c_j}$ is greater than the one saved in the first support array at the position $i$: if it is, we substitute the value in the first one with $\Delta Q_{i\rightarrow c_j}$ and the value in the second one with $c_j$, we don't do anything otherwise. To avoid race condition caused by the two atomic operation (\verb|atomicMax| on the value and eventually \verb|atomicCAS| for the associated community), in the implementation we transform the pair $(c,\Delta Q_{n\rightarrow c})$ of 32 bits variable in a unique word of 64 bit. The half most significant bytes are filled by the value, the other part by the community. Thanks to this, we can use an unique \verb|atomicMax| to substitute both the values. At the end of this step, we have exactly one tuple  $(i, c, \Delta Q_{n\rightarrow c})$ for each node $i$, like the previous algorithm. Now we can update the communities and the pruning support vector similar the other version above.
	\item \textbf{Update Community sub-phase:} This phase update the community just like the one presented in the Prune-Sort-Reduce version (\ref{Prune-Sort-Reduce}, optimization sub-phase \ref{update_com}).
	\item \textbf{Update Pruning sub-phase:} This phase create the array with the pruning information just like the one presented in the Prune-Sort-Reduce version (\ref{Prune-Sort-Reduce}, optimization sub-phase \ref{update_prun}).
\end{enumerate}
We continue to execute this step until the difference in modularity between the configuration drops below a given threshold. The consideration of the computing of $\Delta Q$ in parallel behaviour presented in the previous chapter is still valid. \\
The aggregation phase of this algorithm use once again the map to aggregate, but the key in this context is composed of $(community_source, community_destination)$. We can divide this phase in four sub-phase like the previous version:
\begin{enumerate}
	\item \textbf{Re-indexing communities sub-phase:} This phase created the map that allows the re-indexing of the communities as the one presented in the Prune-Sort-Reduce version (\ref{Prune-Sort-Reduce}, aggregation sub-phase \ref{Re-indexing}).
	\item \textbf{Communities map sub-phase:} In this step, all the tuple of edges $(i, j, w)$ of the original graph are inserted in a hash table. Before the insertion, we transform each entry in the tuple $(r_i, r_j, w)$ where $r_i$ is an index of the community associated with $i$ after the remapping. We use as key the pair $(r_i, r_j)$. At the end of this step, we have each edge of the new graph, because we sum up all the edges that lay between two communities.
	\item \textbf{Contract-sort sub-phase:} At the beginning of this phase, we have a map containing all the edges of the new graph. However, the graph object store the edges information using three ordered vectors, so we have to re-organize the information stored in the unordered and uncoalesced map. To do this, we use the contract operation to transform the map in two vector $(key, value)$ and then we sort the arrays according to the order of the first one. Finally, we update the graph: during this operation, we split the unique composite key in the pair $(source, destination)$ and we copy the value in the relative vector in the graph object. We copy the weight vector without performing an additional operation. 
	\item \textbf{Update variables sub-phase:} This phase update the new graph and the related communities object just like the one presented in the Prune-Sort-Reduce version (\ref{Prune-Sort-Reduce}, aggregation sub-phase \ref{updategraph}).
\end{enumerate}
Like the previous algorithm, this one continues to alternate the two main phases until no further improvement in the modularity update can be obtained.
\subsection{Speed-up the First Iteration in the Optimization Phase}\label{f-1}
In this chapter, we present an optimization technique that we add into our code to speed-up the first iteration of each optimization phase. We include this method in both versions of the algorithm presented previously. We focus this presentation on the Prune-Sort-Reduce version, even if the concept that allows us to optimize the algorithm is still present in the Hashmap version. At the beginning and also after each aggregation phase, we notice that we have a configuration in which each node is assigned to each self-community, i.e. each node $i$ is assigned to the community $i$ and it is the only node assigned to it. In this configuration, when in the copy sub-phase we transform each edge $(i,j)$ in the pair $(i,c_j)$ where $c_j$ is the community of the node $j$, we obtain the same original pair, because the index of $c_j$ is equal to $j$. In addition, considering that each node is assigned to a different community, we don't need the sort and reduce sub-phases, because the pairs $(i, j)$ are already sorted by the construction of the graph object (n.b. we need a sorted vector for the select max sub-phase) and the reduction is useless, because all the pairs have a different composite key $(i, j)$.
Also the self-counting sub-phase is useless, because no edge that isn't a self-loop lay a node in the same community and during the copy sub-phase we excluding the self-loop from the computation. 
Considering all these facts, we choose to remove in the first iteration this three sub-phases and also to avoid the useless transformation in the copy sub-phase. For the Hashmap version of the algorithm, we still use this optimization based on the other version because we use the hashmap to aggregate different edges and this aggregation is no longer necessary.

\subsection{Data Structures}
The two main structures that we need to represent are the original graph and the community structure. Commonly a graph $G(V, E)$ is represented using its adjacency matrix: each node is associated with an incremental id in the range $(0, n-1)$ where $n$ is the number of nodes. To retrieve the weights of an edge between two nodes, we look to the values stored at the position $(id_1, id_2)$. As we say in the chapter \ref{3.1}, the community detection techniques are executed on sparse graph to make sense.  Therefore, if we use a matrix to represent a sparse graph, this matrix will have a lot of zeros and only some values different from it. Even if this pattern allows to retrieve the weight of an edge in constant time, this data structure isn't suitable for a problem that involves millions of data because we need too much memory to allocate this matrix that has several unused values. Therefore we have the necessity of compress the adjacency matrix. To do this, we choose to represent it using a coordinate list (often referred to as COO). We have three vectors \verb|edges_source|,  \verb|edges_destination| and  \verb|weights| that contain respectively the ids of the rows, the ids of the columns and the values. The standard modularity optimization is computed on undirected graphs: this means that the adjacency matrix is symmetric and we can store in the COO list only the upper triangle of the matrix and we still have all the edges represented. Despite this observation, we store all the values of the adjacency matrix because we need those repeated values in these algorithms. In fact, we consider the values in the first array as the source node of the edges and the values in the second one as the destination node of the edges. As we describe in the following section, we transform the vector that contains the destination nodes to a vector that contains the destination community. To have always all edges even in this new behaviour, we have to represent every edge twice in this structure reverting the order of the source and the destination. These vectors are also sorted by the pair $(source, destination)$ and there is a vector named  \verb|neighboorhood_sum| of length $n$ in which at position $i$ there is the starting position of the edges that have source $i$ in \verb|edge_source|. Thanks to this vector, we can retrieve fast all the neighbour of a given node. These particular COO lists are also known as compressed neighbour lists. In the graph object, we also store a vector named \verb| tot_weight_per_nodes| that associate each node $i$ to its degree, the total degree of the graph, the number of nodes and the number of the edges. We use \verb|thrust::device_vector| to store all this data on the GPUs memory. In summary, the graph object is the following:\\
\begin{lstlisting}[language=C++]
struct GraphDevice {
unsigned int n_nodes;
unsigned long n_links;
double total_weight;

thrust::device_vector<unsigned> tot_weight_per_nodes;
thrust::device_vector<unsigned int> neighbourhood_sum;

thrust::device_vector<unsigned int> edge_source;
thrust::device_vector<unsigned int> edge_destination;
thrust::device_vector<unsigned int> weights;
}
\end{lstlisting}
To represent the communities, we use another object that contains the graph associated with it. We notice that the maximum number of different communities is pair to the total number of nodes: this is also the configuration at the beginning of the algorithm.  Considering this, we choose to identify each community with an incremental id in the range $(0, n-1)$, like we do previously with the nodes. Therefore, in the community object, we have a vector named \verb|communities| of length $n$ in which in the position $i$ there is the id of the community of the node $i$. Besides, there is a vector of size $n$ that associate each community to its total weight.  In summary,  the community object is the following:\\
\begin{lstlisting}[language=C++]
struct Community {
GraphDevice graph;

thrust::device_vector<unsigned int> communities;
thrust::device_vector<double> communities_weight;
}
\end{lstlisting}